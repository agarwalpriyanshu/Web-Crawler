# ğŸ•¸ï¸ Multi-threaded Web Crawler in Java

A high-performance web crawler built using **Java**, capable of crawling websites concurrently with user-defined depth and thread count. The crawler extracts hyperlinks from web pages using **Jsoup** and improves performance using **multithreading** with `ExecutorService` and `Phaser`.

---

## ğŸš€ Features

- ğŸ”— Extracts and prints all hyperlinks from a given URL  
- ğŸ” Supports configurable **crawl depth**  
- âš™ï¸ Uses **multiple threads** for faster execution  
- ğŸ§µ Dynamic thread synchronization using `Phaser`  
- âœ… Avoids duplicate URLs using `ConcurrentHashMap`  
- ğŸ“ˆ Achieves up to **10x performance improvement** over a single-threaded version  

---

## ğŸ› ï¸ Tech Stack

- Java  
- Jsoup (HTML parser)  
- ExecutorService & Phaser (Java Concurrency)  
- ConcurrentHashMap & BlockingQueue

---

## ğŸ“‚ Project Structure
<pre>org.example.crawler
â”œâ”€â”€ WebCrawler.java       // Entry point, handles input and thread management
â”œâ”€â”€ CrawlerTask.java      // Runnable task for each crawling operation
â”œâ”€â”€ URLFetcher.java       // Fetches and parses URLs using Jsoup
â”œâ”€â”€ URLStore.java         // Manages visited URLs and the URL queue</pre>


---

## ğŸ§ª How to Run

1. Clone the repository  
2. Ensure you have **Java 8+** and Jsoup dependency set up  
3. Compile and run the `WebCrawler` class

```bash
Enter your URL: https://example.com  
Enter the crawl depth: 2  
Enter the number of worker threads: 5  
```
---

## ğŸ“¸ Sample Output
```
Thread-1: https://example.com
[https://example.com/about]
Thread-2: https://example.com/about
...
Time taken: 350ms
```
---
## ğŸ“š Learning Outcomes

- Gained deep understanding of Java concurrency and synchronization

- Applied thread-safe collections for shared state management

- Explored real-world use of HTML parsing libraries like Jsoup

- Designed a scalable architecture for web crawling systems





